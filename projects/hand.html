<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    
    
      <meta name="author" content="Haimo Zhang">
    

    
      <meta name="description" content="hand-mounted sensors and actuators to digitize daily life experiences">
    

    

    
    
      <title>Hand Augmentation | Haimo Zhang - empowering everyday life experiences</title>
    

    
    <link id="theme" rel="stylesheet" href="/assets/css/theme-radiant.css">
    
    
    <script type="text/javascript">
var theme_brightness = (function() {
  var current_style = 'radiant';
  var theme = document.getElementById("theme")
  function set(brightness) {
    if (brightness === 'radiant' || brightness === 'ambient') {
      var path = theme.getAttribute('href');
      theme.setAttribute('href', path.replace(/radiant|ambient/, brightness));
      current_style = brightness;
    } else {
      throw 'unknown brightness value: "' + brightness + '"';
    }
  }
  function set_by_time() {
    var h = (new Date()).getHours();
    set(h >= 6 && h < 18 ? 'radiant' : 'ambient');
  }
  function toggle() {
    if (current_style === 'radiant') {
      set('ambient');
    } else {
      set('radiant');
    }
  }
  set_by_time();  // default behavior is to set theme brightness based on time of day
  return {
    get_current: () => current_style,
    set: set,
    set_by_time: set_by_time,
    toggle: toggle
  };
})();
    </script>

    
    

  </head>
  <body>

    <div class="panel panel-header nav-bar-container"><div class="nav-bar">
<nav>
  
    
      
        <a href="/index.html">about</a>
      
    
  
    
      
        <a href="/projects.html">projects</a>
      
    
  
    
      
        <a href="/publications.html">publications</a>
      
    
  
    
  

  

</nav>
</div>
</div>

    <div class="panel panel-body">

      <div class="panel panel-sidebar panel-sidebar-left">
      </div>

      <div class="panel panel-content"><div class="project-title-container">
  <h1 class="project-title">Hand Augmentation</h1>
  <h2 class="project-subtitle">hand-mounted sensors and actuators to digitize daily life experiences</h2>
</div>

<div class="project-excerpt-container">
  <p class="project-excerpt">Our hands is an important interface through which we interact with the external world. Augmenting the hand with sensors and actuators allows digitization of such analog interactions, which enhances our daily life experiences.</p>
</div>

<h2 id="understanding-object-and-activity-from-hand-posture-and-action">Understanding object and activity from hand posture and action</h2>

<p>Holding an object, the hand postures and actions
reflect the object shape and how it is being manipulated.
In the <em>OSense</em> project, we used finger-mounted
motion sensors to validate this hypothesis
and showed its use cases in daily lives.</p>

<p>We tested the following activities:</p>

<p><img src="/assets/projects/hand//osense-activities.png" alt="activities investigated in OSense" /></p>

<p>Such activities could be used to understand the context of the user,
as demonstrated below:</p>

<p><img src="/assets/projects/hand//osense-applications.png" alt="applications of OSense" /></p>

<ul style="list-style-type:none;">
  <li>a) tracking time spent on writing.</li>
  <li>b) tracking how many times a user drank water.</li>
  <li>c) &amp; d) combined with location information,
         detecting when a person leaves home or office.</li>
  <li>e) tracking the use of different tools and performing different
         actions when preparing a meal.</li>
</ul>

<p>The following video explains the <em>OSense</em> project:</p>

<video controls="">
  <source src="/assets/projects/hand//osense.mp4" type="video/mp4" />
  Your browser does not support the video tag.
  Download the video <a href="/assets/projects/hand//osense.mp4">here</a>.
</video>

<h2 id="force-aware-gesture-interactions">Force-aware gesture interactions</h2>

<p>Different forces could be applied when performing gestures,
such as holding a ball in hand.
In the <em>fSense</em> project, we use the heart rate sensor
available in many smartwatches to sense the force
while performing various hand gestures.
This allowed for new ways to design gestural interfaces,
taking force as another input dimension.</p>

<p>We collected data representing three user-elicited force levels
while performing the following gestures:</p>

<p><img src="/assets/projects/hand//fsense-gestures.png" alt="gestures used in the fSense study" /></p>

<p>This allowed us to train a classifier of force,
independent of the type of the performed gesture.</p>

<p>The applications of force-aware gesture as exemplified below:</p>

<p><img src="/assets/projects/hand//fsense-applications.png" alt="applications of fSense" /></p>

<ul style="list-style-type:none;">
  <li>a) taking selfies using fist clench.</li>
  <li>b) supporting force-aware stylus interactions,
         e.g., in a text annotation application.</li>
  <li>c) answering calls while driving.</li>
  <li>d) answering calls with "busy hands".</li>
  <li>e) navigating and selecting items in a menu,
         by combining with existing motion sensors of the smartwatch.</li>
</ul>

<p>The following video introduces <em>fSense</em>.</p>

<video controls="">
  <source src="/assets/projects/hand//fsense.mp4" type="video/mp4" />
  Your browser does not support the video tag.
  Download the video <a href="/assets/projects/hand//fsense.mp4">here</a>.
</video>

<h2 id="related-publications">Related Publications</h2>
<div class="publication-group-container">
    <div class="publication-group-content-container"><div class="publication-container">
  <span class="publication">

    <span class="title">fSense: Unlocking the Dimension of Force for Gestural Interactions Using Smartwatch PPG Sensor.</span>

    <span class="authors"><span class="author ">
          <span class="given-name">
            Thisum
          </span>
          <span class="family-name">
            Buddhika,</span>
        </span><span class="author myself">
          <span class="given-name">
            Haimo
          </span>
          <span class="family-name">
            Zhang,</span>
        </span><span class="author ">
          <span class="given-name">
            Samantha W. T.
          </span>
          <span class="family-name">
            Chan,</span>
        </span><span class="author ">
          <span class="given-name">
            Vipula
          </span>
          <span class="family-name">
            Dissanayake,</span>
        </span><span class="author ">
          <span class="given-name">
            Suranga
          </span>
          <span class="family-name">
            Nanayakkara,</span>
        </span>and<span class="author ">
          <span class="given-name">
            Roger
          </span>
          <span class="family-name">
            Zimmermann.</span>
        </span></span>in
      <span class="venue">
        Proceedings of the 10th Augmented Human International Conference 2019.
      </span><span class="year">
        2019.
      </span><span class="publisher">
        ACM.
      </span><span class="place">
        New York, NY, USA.
      </span><span class="resources">
      <a href="/assets/papers/buddhika2019fsense.pdf">
        ðŸ“ƒ
        <!-- some suitable unicode characters
          ðŸ“œ
          ðŸ“ƒ
          ðŸ“„
        -->
      </a>/
        <a href="/assets/videos/buddhika2019fsense.mp4">
          ðŸŽ¥
          <!-- some suitable unicode characters
            ðŸŽ¥
            ðŸ“º
          -->
        </a></span>

  </span></div>
<div class="publication-container">
  <span class="publication">

    <span class="title">OSense: Object-activity Identification Based on Gasping Posture and Motion.</span>

    <span class="authors"><span class="author ">
          <span class="given-name">
            Thisum
          </span>
          <span class="family-name">
            Buddhika,</span>
        </span><span class="author myself">
          <span class="given-name">
            Haimo
          </span>
          <span class="family-name">
            Zhang,</span>
        </span><span class="author ">
          <span class="given-name">
            Chamod
          </span>
          <span class="family-name">
            Weerasinghe,</span>
        </span><span class="author ">
          <span class="given-name">
            Suranga
          </span>
          <span class="family-name">
            Nanayakkara,</span>
        </span>and<span class="author ">
          <span class="given-name">
            Roger
          </span>
          <span class="family-name">
            Zimmermann.</span>
        </span></span>in
      <span class="venue">
        Proceedings of the 10th Augmented Human International Conference 2019.
      </span><span class="year">
        2019.
      </span><span class="publisher">
        ACM.
      </span><span class="place">
        New York, NY, USA.
      </span><span class="resources">
      <a href="/assets/papers/buddhika2019osense.pdf">
        ðŸ“ƒ
        <!-- some suitable unicode characters
          ðŸ“œ
          ðŸ“ƒ
          ðŸ“„
        -->
      </a>/
        <a href="/assets/videos/buddhika2019osense.mp4">
          ðŸŽ¥
          <!-- some suitable unicode characters
            ðŸŽ¥
            ðŸ“º
          -->
        </a></span>

  </span></div>
</div>
  </div>

</div>

      <div class="panel panel-sidebar panel-sidebar-right">
      </div>

    </div>

    <div class="panel panel-footer">
    </div>

  </body>
</html>
